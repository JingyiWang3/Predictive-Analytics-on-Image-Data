return(fit.model)
}
system.time(fit.model <- train(rgb_train, label_train, run.gbm = T))
system.time(fit.model <- train(sift_train, label_train, run.gbm = T))
.gbm = T)
mean(pred == label_test)
pred <- test(fit.model, sift_test, run.gbm = T)
mean(pred == label_test)
mean(pred != label_test)
#########################################################
### Train a classification model with training images ###
#########################################################
### Author: Group 9
### ADS Spring 2018
train <- function(dat_train, label_train,
run.xgboost = F, run.gbm = F,
run.adaboost = F,
par=NULL){
### train a model according to input
### Input:
###   dat_train -  processed features from images
###   label_train -  class labels for training images
###   run.xxxxxx - select which model to fit
###   par - specified parameters, otherwise use default
### Output:
###   fitted model
### load libraries
library("gbm")
library("adabag")
library("xgboost")
### fit selected model
#### gradient boosting model
if(run.gbm == T){
if(is.null(par)){
ntrees = 100
shrinkage = 0.1
}
else{
ntrees = par$ntrees
shrinkage = par$shrinkage
}
fit.model <- gbm.fit(x = dat_train,
y = label_train,
interaction.depth = 3,
shrinkage = shrinkage,
bag.fraction = 0.5,
n.trees = ntrees,
verbose = FALSE,
distribution="multinomial")
}
###############################################################
####  adaboosting model
if(run.adaboost == T){
# load parameter
if(is.null(par)){
mfinal <- 100
} else {
mfinal <- par$mfinal
}
# convert trainning data to data frame
train <- data.frame(label = factor(label_train), data = dat_train)
# fit model
fit.model <- boosting(label~.,data = train,
mfinal = mfinal,
coeflearn= "Zhu")
}
###############################################################
#### xgboost model
if(run.xgboost == T){
if(is.null(par)){
depth <- 5
child_weight <- 3
} else {
depth <- par$max.depth
child_weight <- par$min_child_weight
}
# create xgb.Dmatrix
train_matrix <- xgb.DMatrix(data=data.matrix(dat_train),label=label_train)
# fit xgboost model
fit.model <- xgb.train(data = train_matrix,
max.depth = depth,
min_child_weight = child_weight,
eta = 0.3,
nthread = 4,
nround = 100,
num_class = 3)
}
return(fit.model)
}
system.time(fit.model <- train(sift_train, label_train, run.gbm = T))
pred <- test(fit.model, sift_test, run.gbm = T)
mean(pred != label_test)
# gist
system.time(fit.model <- train(gist_train, label_train, run.gbm = T))
# user  system elapsed
# 13.567   0.132  13.821
pred <- test(fit.model, gist_test, run.gbm = T)
mean(pred != label_test)
# rgb
system.time(fit.model <- train(rgb_train, label_train, run.gbm = T))
# user  system elapsed
# 20.702   0.211  21.283
pred <- test(fit.model, rgb_test, run.gbm = T)
mean(pred != label_test)
########################
### Feature Turning ###
########################
### Author: Group 9
### ADS Spring 2018
tune <- function(dat_train ,label_train,
run.xgboost = F,
run.gbm = F,
run.adaboost = F,
verbose = FALSE){
### tune parameter
### Input:
###   dat_train -  processed features from images
###   label_train -  class labels for training images
###   run.xxxxxx - select which model to fit
###   verbose - TRUE means print cv error while every loop
### Output:
###   best parameter
### load libraries
library("gbm")
library("adabag")
library("xgboost")
### load functions
source("./lib/cross_validation.R")
#############################################
## adaboost model tune parameter
if(run.adaboost == T){
## parameter pool
mfinal <- c(50, 75, 100, 125)
## initial cv error
cv.error <- c()
## loop parameter combination
for(i in length(mfinal)){
par <- list(mfinal = mfinal[i])
cv.error[i] <- cv(dat_train, label_train, run.adaboost = T, par = par)$error
}
# best cv.error
cv_error =  min(cv.error)
# best parameter
best.mfinal = mfinal[which(cv.error == min(cv.error))]
best_par = list(mfinal = best.mfinal)
}
#############################################
## gbm model tune parameter
if(run.gbm == T){
## parameter pool
shrinks_range <- c(0.01,0.05,0.1,0.15,0.2)
trees_range  <- c(40,50,60,70,100)
## initial cv error
error_matrix = matrix(NA,nrow = length(shrinks_range), length(trees_range))
rownames(error_matrix) <- paste(shrinks_range)
colnames(error_matrix) <- paste(trees_range)
## loop parameter combination
for (i in 1:length(shrinks_range)){
for (j in 1:length(trees_range)){
par <- list(shrinkage = shrinks_range[i], ntrees = trees_range[j] )
error_matrix[i,j] <- cv(dat_train, label_train, run.gbm= T, par = par)$error
}
}
# best cv.error
cv_error =  min(error_matrix)
# best parameter
best_par = list(shrinkage = shrinks_range[which(error_matrix == min(error_matrix), arr.ind = T)[1]],
ntrees = trees_range[which(error_matrix == min(error_matrix), arr.ind = T)[2]])
}
#############################################
## xgboost model tune parameter
if(run.xgboost == T){
max_depth_values <- seq(3,9,2)
min_child_weight_values <- seq(1,6,2)
# error matrix
error_matrix = matrix(NA,nrow = length(max_depth_values), length(min_child_weight_values))
rownames(error_matrix) <- paste(max_depth_values)
colnames(error_matrix) <- paste(min_child_weight_values)
#tuning process
for (i in 1:length(max_depth_values)){
for (j in 1:length(min_child_weight_values)){
par <- list(depth = max_depth_values[i], child_weight = min_child_weight_values[j] )
error_matrix[i,j] <- cv(dat_train, label_train, run.xgboost = T, par = par)$error
}
}
# best cv.error
cv_error =  min(error_matrix)
# best parameter
best_par = list(depth = max_depth_values[which(error_matrix == min(error_matrix), arr.ind = T)[1]],
child_weight = min_child_weight_values[which(error_matrix == min(error_matrix), arr.ind = T)[2]])
}
if(verbose == TRUE){
print(error_matrix)
}
return(list(cv_error,best_par))
}
########################
### Feature Turning ###
########################
### Author: Group 9
### ADS Spring 2018
tune <- function(dat_train ,label_train,
run.xgboost = F,
run.gbm = F,
run.adaboost = F,
verbose = FALSE){
### tune parameter
### Input:
###   dat_train -  processed features from images
###   label_train -  class labels for training images
###   run.xxxxxx - select which model to fit
###   verbose - TRUE means print cv error while every loop
### Output:
###   best parameter
### load libraries
library("gbm")
library("adabag")
library("xgboost")
### load functions
source("./lib/cross_validation.R")
#############################################
## adaboost model tune parameter
if(run.adaboost == T){
## parameter pool
mfinal <- c(50, 75, 100, 125)
## initial cv error
cv.error <- c()
## loop parameter combination
for(i in length(mfinal)){
par <- list(mfinal = mfinal[i])
cv.error[i] <- cv(dat_train, label_train, run.adaboost = T, par = par)$error
}
# best cv.error
cv_error =  min(cv.error)
# best parameter
best.mfinal = mfinal[which(cv.error == min(cv.error))]
best_par = list(mfinal = best.mfinal)
}
#############################################
## gbm model tune parameter
if(run.gbm == T){
## parameter pool
shrinks_range <- c(0.01,0.05,0.1,0.15,0.2)
trees_range  <- c(40,50,60,70,100)
## initial cv error
error_matrix = matrix(NA,nrow = length(shrinks_range), length(trees_range))
rownames(error_matrix) <- paste(shrinks_range)
colnames(error_matrix) <- paste(trees_range)
## initial cv sd
sd_matrix = matrix(NA,nrow = length(shrinks_range), length(trees_range))
rownames(sd_matrix) <- paste(shrinks_range)
colnames(sd_matrix) <- paste(trees_range)
## loop parameter combination
for (i in 1:length(shrinks_range)){
for (j in 1:length(trees_range)){
par <- list(shrinkage = shrinks_range[i], ntrees = trees_range[j] )
error_matrix[i,j] <- cv(dat_train, label_train, run.gbm= T, par = par)$error
sd_matrix[i,j] <- cv(dat_train, label_train, run.gbm= T, par = par)$sd
}
}
# best cv.error
cv_error =  min(error_matrix)
# best parameter
best_par = list(shrinkage = shrinks_range[which(error_matrix == min(error_matrix), arr.ind = T)[1]],
ntrees = trees_range[which(error_matrix == min(error_matrix), arr.ind = T)[2]])
}
#############################################
## xgboost model tune parameter
if(run.xgboost == T){
max_depth_values <- seq(3,9,2)
min_child_weight_values <- seq(1,6,2)
# error matrix
error_matrix = matrix(NA,nrow = length(max_depth_values), length(min_child_weight_values))
rownames(error_matrix) <- paste(max_depth_values)
colnames(error_matrix) <- paste(min_child_weight_values)
## cv sd matrix
sd_matrix = matrix(NA,nrow = length(max_depth_values), length(min_child_weight_values))
rownames(sd_matrix) <- paste(max_depth_values)
colnames(sd_matrix) <-  paste(min_child_weight_values)
#tuning process
for (i in 1:length(max_depth_values)){
for (j in 1:length(min_child_weight_values)){
par <- list(depth = max_depth_values[i], child_weight = min_child_weight_values[j] )
error_matrix[i,j] <- cv(dat_train, label_train, run.xgboost = T, par = par)$error
sd_matrix[i,j] <- cv(dat_train, label_train, run.xgboost = T, par = par)$sd
}
}
# best cv.error
cv_error =  min(error_matrix)
# best parameter
best_par = list(depth = max_depth_values[which(error_matrix == min(error_matrix), arr.ind = T)[1]],
child_weight = min_child_weight_values[which(error_matrix == min(error_matrix), arr.ind = T)[2]])
}
if(verbose == TRUE){
print(error_matrix)
print(sd_matrix)
}
return(list(cv_error,best_par))
}
# create rgb label
rgb_label <- c(rep(0,1000),rep(1,1000),rep(2,1000))
##################################
# set RGB to 8 8 8
par888 = list(nR = 8, nG = 8, nB = 8)
# extract rgb888 feature
rgb888 <- feature(par = par888)
# load feature extraction function and some other files
source("./lib/feature.R")
# extract rgb888 feature
rgb888 <- feature(par = par888)
load("/Users/fanerror/GitHub/Project3_PoodleKFC/rgb.RData")
tune(rgb888,rgb_label,
run.xgboost = T,
verbose = T)
########################
### Feature Turning ###
########################
### Author: Group 9
### ADS Spring 2018
tune <- function(dat_train ,label_train,
run.xgboost = F,
run.gbm = F,
run.adaboost = F,
verbose = FALSE){
### tune parameter
### Input:
###   dat_train -  processed features from images
###   label_train -  class labels for training images
###   run.xxxxxx - select which model to fit
###   verbose - TRUE means print cv error while every loop
### Output:
###   best parameter
### load libraries
library("gbm")
library("adabag")
library("xgboost")
### load functions
source("./lib/cross_validation.R")
#############################################
## adaboost model tune parameter
if(run.adaboost == T){
## parameter pool
mfinal <- c(50, 75, 100, 125)
## initial cv error
cv.error <- c()
## loop parameter combination
for(i in length(mfinal)){
par <- list(mfinal = mfinal[i])
cv.error[i] <- cv(dat_train, label_train, run.adaboost = T, par = par)$error
}
# best cv.error
cv_error =  min(cv.error)
# best parameter
best.mfinal = mfinal[which(cv.error == min(cv.error))]
best_par = list(mfinal = best.mfinal)
}
#############################################
## gbm model tune parameter
if(run.gbm == T){
## parameter pool
shrinks_range <- c(0.01,0.05,0.1,0.15,0.2)
trees_range  <- c(40,50,60,70,100)
## initial cv error
error_matrix = matrix(NA,nrow = length(shrinks_range), length(trees_range))
rownames(error_matrix) <- paste(shrinks_range)
colnames(error_matrix) <- paste(trees_range)
## initial cv sd
sd_matrix = matrix(NA,nrow = length(shrinks_range), length(trees_range))
rownames(sd_matrix) <- paste(shrinks_range)
colnames(sd_matrix) <- paste(trees_range)
## loop parameter combination
for (i in 1:length(shrinks_range)){
for (j in 1:length(trees_range)){
par <- list(shrinkage = shrinks_range[i], ntrees = trees_range[j] )
error_matrix[i,j] <- cv(dat_train, label_train, run.gbm= T, par = par)$error
sd_matrix[i,j] <- cv(dat_train, label_train, run.gbm= T, par = par)$sd
}
}
# best cv.error
cv_error =  min(error_matrix)
# best parameter
best_par = list(shrinkage = shrinks_range[which(error_matrix == min(error_matrix), arr.ind = T)[1]],
ntrees = trees_range[which(error_matrix == min(error_matrix), arr.ind = T)[2]])
}
#############################################
## xgboost model tune parameter
if(run.xgboost == T){
max_depth_values <- seq(3,9,2)
min_child_weight_values <- seq(1,6,2)
# error matrix
error_matrix = matrix(NA,nrow = length(max_depth_values), length(min_child_weight_values))
rownames(error_matrix) <- paste(max_depth_values)
colnames(error_matrix) <- paste(min_child_weight_values)
## cv sd matrix
sd_matrix = matrix(NA,nrow = length(max_depth_values), length(min_child_weight_values))
rownames(sd_matrix) <- paste(max_depth_values)
colnames(sd_matrix) <-  paste(min_child_weight_values)
#tuning process
for (i in 1:length(max_depth_values)){
for (j in 1:length(min_child_weight_values)){
print(1)
par <- list(depth = max_depth_values[i], child_weight = min_child_weight_values[j] )
print(2)
error_matrix[i,j] <- cv(dat_train, label_train, run.xgboost = T, par = par)$error
print(3)
sd_matrix[i,j] <- cv(dat_train, label_train, run.xgboost = T, par = par)$sd
}
}
# best cv.error
cv_error =  min(error_matrix)
# best parameter
best_par = list(depth = max_depth_values[which(error_matrix == min(error_matrix), arr.ind = T)[1]],
child_weight = min_child_weight_values[which(error_matrix == min(error_matrix), arr.ind = T)[2]])
}
if(verbose == TRUE){
print(error_matrix)
print(sd_matrix)
}
return(list(cv_error,best_par))
}
tune(rgb888,rgb_label,
run.xgboost = T,
verbose = T)
#########################################################
### Train a classification model with training images ###
#########################################################
### Author: Group 9
### ADS Spring 2018
train <- function(dat_train, label_train,
run.xgboost = F, run.gbm = F,
run.adaboost = F,
par=NULL){
### train a model according to input
### Input:
###   dat_train -  processed features from images
###   label_train -  class labels for training images
###   run.xxxxxx - select which model to fit
###   par - specified parameters, otherwise use default
### Output:
###   fitted model
### load libraries
library("gbm")
library("adabag")
library("xgboost")
### fit selected model
#### gradient boosting model
if(run.gbm == T){
if(is.null(par)){
ntrees = 100
shrinkage = 0.1
}
else{
ntrees = par$ntrees
shrinkage = par$shrinkage
}
fit.model <- gbm.fit(x = dat_train,
y = label_train,
interaction.depth = 3,
shrinkage = shrinkage,
bag.fraction = 0.5,
n.trees = ntrees,
verbose = FALSE,
distribution="multinomial")
}
###############################################################
####  adaboosting model
if(run.adaboost == T){
# load parameter
if(is.null(par)){
mfinal <- 100
} else {
mfinal <- par$mfinal
}
# convert trainning data to data frame
train <- data.frame(label = factor(label_train), data = dat_train)
# fit model
fit.model <- boosting(label~.,data = train,
mfinal = mfinal,
coeflearn= "Zhu")
}
###############################################################
#### xgboost model
if(run.xgboost == T){
if(is.null(par)){
depth <- 5
child_weight <- 3
} else {
depth <- par$depth
child_weight <- par$child_weight
}
# create xgb.Dmatrix
train_matrix <- xgb.DMatrix(data=data.matrix(dat_train),label=label_train)
# fit xgboost model
fit.model <- xgb.train(data = train_matrix,
max.depth = depth,
min_child_weight = child_weight,
eta = 0.3,
nthread = 4,
nround = 100,
num_class = 3)
}
return(fit.model)
}
tune(rgb888,rgb_label,
run.xgboost = T,
verbose = T)
tune(rgb101010,rgb_label,
run.xgboost = T,
verbose = T)
##################################
# set RGB to 10 12 12
par101212 = list(nR = 10, nG = 12, nB = 12)
